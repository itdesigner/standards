<!DOCTYPE html>
<html>
    <head>
        <title>Engineering Standards : Design for Failure</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">Engineering Standards</a></span>
                            </li>
                                                    <li>
                                <span><a href="33470365.html">Engineering Standards &amp; Best Practices</a></span>
                            </li>
                                                    <li>
                                <span><a href="Architectural-Documentation_33471477.html">Architectural Documentation</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Engineering Standards : Design for Failure
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                        
        
    
        
    
        
        
            Created by <span class='author'> Jerry Shaughnessy</span>, last modified on Oct 28, 2017
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <div class="contentLayout2">
<div class="columnLayout two-right-sidebar" data-layout="two-right-sidebar">
<div class="cell normal" data-type="normal">
<div class="innerCell">
<div class="ViewPageHeader_content_Y3y"><div><div><div><div><div><div><div class="page view" style="margin-left: 0.0px;"><div class="wiki-content">Assuming things will fail, ensure you carefully review every aspect of your cloud architecture and design for failure scenarios against each one of them. In particular, assume hardware will fail, cloud data center outages will happen, database failure or performance degradation will occur, expected volumes of transactions will be exceeded, and so on. In addition, in an auto-scaled environment, for example, nodes may be shutdown in response to loads getting back to normal levels after a spike. Nodes might be rebooted by the cloud platform. There can also be unexpected application failures. In all cases, the design goal should be to handle such error conditions gracefully and minimize any impact to the user experience.</div><h2 class="wiki-content" id="DesignforFailure-EverythingFails">Everything Fails</h2><div class="wiki-content">Everything fails all the time: Hard disks break, computers overheat, wires get broken, the power goes out, earthquakes damage buildings, and because of all this, no single device should be considered fault-tolerant.<br/><br/>It is a guarantee that something in your setup will eventually fail. At large enough scale, something is always failing. <br/><br/>The reasoning is simple: hardware is physical—subject to the laws of physics—and thus, it breaks all the time. This is a mantra that anyone designing software systems in the AWS cloud must always keep in mind. It is also one of the design principles on which the Amazon AWS cloud solutions are built, and it is a principle that AWS does not eliminate for its customers. <br/><br/>Something or someone has to deal with all these uncertainties. Ideally, cloud solutions providers such as Amazon AWS would solve problems transparently for their customers and would take care of providing a failure-proof environment, but that’s just not how it works.<br/><br/>As Reed Hastings said during the AWS Re:Invent 2012 Keynote in Las Vegas, “We are still in the assembly language phase of cloud computing.” Coping with failure is for the most part left in the hands of cloud computing users, and software architects must take these inevitable failures very seriously. Design for failure must be deliberate, where there is an assumption that the components in a system will in fact fail, and the best plans design backwards to address them. <br/><br/>When this backwards planning is done, an important advantage is discovered: Designing a solution for reliability leads to a path that, with little additional cost and effort, solves scalability issues as well.</div><div class="wiki-content"><br/>I recently had the opportunity to attend an AWS online session and a short presentation given by their team on Designing for Failure. It opened my eyes to the reality of application design when dealing with failure or even basic exception handling.One of the defining characteristics between a good developer and a great one is how they deal with failures. Good developers will handle the obvious examples in their code – checking for unexpected input, catching library exceptions, and sometimes edge cases. Why do we build resilient applications and what about the end user?</div><h2 class="wiki-content" id="DesignforFailure-WhyBuildResilientApplications?">Why Build Resilient Applications?</h2><div class="wiki-content">There are two main reasons that we design applications for failure.The first reason is User Experience. It’s no secret that you will have user attrition and lost revenue if you cannot shield your end users from issues outside their control. The second reason is Business Services. All business critical systems require resiliency and the difference between a 99.7% uptime and 99.99% could be hours of lost revenue or interrupted business services.Given an application load of 1 billion requests per month, a 99.7% downtime is 2+ hours versus just 4 minutes for 99.99%. Ouch!</div><div class="wiki-content">Werner Vogels, the CTO of Amazon Web Services once said at a previous re:Invent “Everything fails, all the time.” It’s a devastating reality and it’s something we all must accept. No matter how mathematically improbable, we simply cannot eliminate all failures. It’s how we reduce the impact of those failures that improves the overall resiliency of our applications.</div><h2 class="wiki-content" id="DesignforFailure-CorePrinciplesforFailure">Core Principles for Failure</h2><h3 class="wiki-content" id="DesignforFailure-GracefulDegredation">Graceful Degredation</h3><div class="wiki-content">The way we reduce the impact of failure on our users and business is through graceful degradation. Conceptually it’s very simple – we want to continue to operate in lieu of a failure in some degraded capacity. Keeping with the premise that applications fail all the time, you’ve probably experienced degraded services without even realizing it – and that is the ultimate goal.</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-Caching">Caching</h4><div class="wiki-content" style="margin-left: 30.0px;">Caching is the first layer of defense when dealing with a failure. Depending on your applications reliance on up-to-date bleeding edge information you should consider caching everything. It’s very easy for developers to reject caching because they always want the freshest information for their users. However, when the difference between a happy customer and a sad one is using some few-minute old data… choose the latter.As an example, imagine you have a fairly advanced web application. What can you cache?</div><div class="wiki-content" style="margin-left: 30.0px;"><ul style="margin-left: 0.0px;"><li style="margin-left: 0.0px;">Full HTML pages with CloudFront</li><li>Database records with ElastiCache</li><li>Page Fragments with tools such as Varnish</li><li>Remote API calls from your backend with ElastiCache</li></ul></div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-Retry">Retry</h4><div class="wiki-content" style="margin-left: 30.0px;">As applications get more complex we rely on more external services than ever before. Whether it’s a 3rd party service provider or your microservices architecture at work, failures are common and often transient. A common pattern for dealing with transient failures on these types of requests is to implement retry logic. Using <a class="external-link" href="https://en.wikipedia.org/wiki/Exponential_backoff" rel="nofollow">exponential back off</a> or a <a class="external-link" href="https://en.wikipedia.org/wiki/Fibonacci_number" rel="nofollow">Fibonacci sequence</a> you can retry for some time before eventually throwing an exception. It’s important to fail fast and not trigger rate limiting on your source, so don’t continue indefinitely.</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-RateLimiting">Rate Limiting</h4><div class="wiki-content" style="margin-left: 30.0px;">In the case of denial of service attacks, self-imposed or otherwise, your primary defense is rate limiting based on a context. You can limit the amount of requests to your application based on user data, source address or both. By imposing a limit on requests you can improve your performance during a failure by reducing the actual load and the load imposed by your retry logic. Also consider using exponential back off or a Fibonacci increase to help mitigate particularly demanding services.For example, during a peak in capacity that cannot be met immediately, a reduction in load would allow your applications infrastructure to respond to the demand (think auto scaling) before completely failing.</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-FailFast">Fail Fast</h4><div class="wiki-content" style="margin-left: 30.0px;">When your application is running out of memory, threads or other resources you can help recovery time by failing fast. You should return an error as soon as possible when it’s detected. Not only will your users be happier not waiting on your application to respond, you will also not cascade the delay into dependent services.</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-StaticFallback">Static Fallback</h4><div class="wiki-content" style="margin-left: 30.0px;">Whether you’re rate limiting or simply cannot fail silently, you’ll need something to fallback to. A static fallback is a way to provide at least some response to your end users without leaving them to the wind with erroneous error output or no response at all. It’s always better to return content that makes sense to the context of the user and you’ve probably seen this before if you’re a frequent user of sites like Reddit or Twitter.In the case of our example web application, you can configure Route53 to fallback to HTML pages and assets served from Amazon S3 with very little headache. You could set this up today!</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-FailSilently">Fail Silently</h4><div class="wiki-content" style="margin-left: 30.0px;">When all of your layers of protection have failed to preserve your service, it’s time to fail silently. Failing silently is when you rely on your logging, monitoring and other infrastructure to respond to your errors with the least impact to the end user. It’s a best practice to return a 200 OK with no content and log your errors on the backend than to return a 500 Internal Server Error, similar HTTP status code or worse yet, a nasty stack trace/log dump.</div><h3 class="wiki-content" id="DesignforFailure-FailingFastandYou">Failing Fast and You</h3><div class="wiki-content">There are two patterns that you can implement to improve your ability to fail fast: Circuit Breaking and Load Shedding. Generally, you want to leverage your monitoring tools such as Cloudwatch and your logs to detect failure early and begin mitigating the impact as soon as possible. These two patterns are automation at it’s finest.</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-CircuitBreaking">Circuit Breaking</h4><div class="wiki-content" style="margin-left: 30.0px;">Circuit breaking is purposefully degrading performance in light of failure events in your logging or monitoring system. You can utilize any of the degradation patterns mentioned above in the circuit. Finally, by implementing health checks into your service you can restore normal service as soon as possible.</div><h4 class="wiki-content" style="margin-left: 30.0px;" id="DesignforFailure-LoadShedding">Load Shedding</h4><div class="wiki-content" style="margin-left: 30.0px;">Load shedding is a method of failing fast that occurs at the networking level. Like circuit breaking, you can rely on monitoring data to reroute traffic from your application to a Static Fallback that you have configured. For example, Route53 has failover support built right in that would allow you to use this pattern right away.</div><h2 class="wiki-content" id="DesignforFailure-Sample(althoughsimplistic)Scenario">Sample (although simplistic) Scenario</h2><div class="wiki-content">For example, let’s say that you have a simple web application where customers can log in from the Internet. The application collects data from those individuals, stores it, and provides reports based on that data. <br/><br/>Initially, you could design it to run very simply across one server and one instance of MySql. It would look something like this:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://4.bp.blogspot.com/-HMXrR_rByLI/U0wnqzjDMJI/AAAAAAAAAug/-MOiuy9Q538/s1600/fig1.png" data-image-src="https://4.bp.blogspot.com/-HMXrR_rByLI/U0wnqzjDMJI/AAAAAAAAAug/-MOiuy9Q538/s1600/fig1.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig1</td></tr></tbody></table></div></div><div class="separator">This solution might work for a while, and could even run on the computer under your desk with minimal initial cost. Unfortunately though, this scenario includes several concerns: <br/><br/><ol><li>It doesn’t scale. As traffic increases you’ll quickly reach a breaking point. Either the database (DB) machine or the product server is bound to get saturated and stop working properly.</li><li>There are only two physical components and they depend on each other. If either of them fails, the system is going to be down.</li></ol><br/>So let’s start by focusing on the component that is further away from the customer, and move backwards from there. What happens if the MySql DB starts failing? I’ll indicate with a red color a component that represents a single point of failure, with green a component that has been determined fault-tolerant, and with white a component that I haven’t yet examined. Our goal is to make all the boxes green. With that in mind:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://1.bp.blogspot.com/-oVUT6VCWLuY/U0wn0W5QahI/AAAAAAAAAuo/TW5VD0W8kTg/s1600/fig2.png" data-image-src="https://1.bp.blogspot.com/-oVUT6VCWLuY/U0wn0W5QahI/AAAAAAAAAuo/TW5VD0W8kTg/s1600/fig2.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig2</td></tr></tbody></table></div></div><div class="separator">In this design if the database fails:<br/><br/><ol><li>Customers can no longer log in because their credentials are stored in the DB.</li><li>The system is not able to collect data since there is no place to put it.</li><li>The business logic cannot operate because the data is not available.</li><li>The system is unable to display reports since the data is inaccessible.</li></ol><br/>In other words, the product becomes completely non-functional, resulting in total disaster:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://3.bp.blogspot.com/-uOlU6fcVETE/U0wn9ybeSuI/AAAAAAAAAuw/c_bK7Y129EU/s1600/fig3.png" data-image-src="https://3.bp.blogspot.com/-uOlU6fcVETE/U0wn9ybeSuI/AAAAAAAAAuw/c_bK7Y129EU/s1600/fig3.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig3</td></tr></tbody></table></div></div><div class="separator">Failure started from the DB and moved backward toward the customer, ultimately halting the experience. <br/><br/>How can this be avoided? We can prevent this scenario by assuming during the design of the system that any MySql instance could fail at any time. We need to replace that single point of failure with something that is tolerant of errors. There are many options to achieve this result. <br/><br/>For example, we could partition the data in several MySql instances. With this schema, we would put the population of customers in buckets and use a separate MySql instance for each of the buckets. If any of the instances goes bad, only the group of customers stored in the “bad bucket” will experience issues.<br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://3.bp.blogspot.com/-8tGtuBZoQPU/U0woFGvdcyI/AAAAAAAAAu4/5xZcPl9BEY0/s1600/fig4.png" data-image-src="https://3.bp.blogspot.com/-8tGtuBZoQPU/U0woFGvdcyI/AAAAAAAAAu4/5xZcPl9BEY0/s1600/fig4.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig4</td></tr></tbody></table></div></div><div class="separator"><br/>In this simple example we have two buckets. Assuming that each customer has a numeric customer ID, we could assign customers with odd ID numbers to DB1 and customers with even ID numbers to DB2. If one of the two MySql instances goes bad, you’d get this situation:<br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://3.bp.blogspot.com/-dNgBoC5nrCk/U0woMsWgIcI/AAAAAAAAAvA/aqJLO0bH_DI/s1600/fig5.png" data-image-src="https://3.bp.blogspot.com/-dNgBoC5nrCk/U0woMsWgIcI/AAAAAAAAAvA/aqJLO0bH_DI/s1600/fig5.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig5</td></tr></tbody></table></div></div><div class="separator">While this is not ideal, at least the product will continue working for 50 percent of your customers while you resolve the problem. In general, if you have n buckets and one goes bad, the system would be broken for only 1/n of the customers. The more buckets that exist, the better this works.<br/><br/>However, there are several drawbacks to this design. The first of which is that data partitioned this way becomes costly to repartition. For instance, if we start with 3 buckets and we want to move to 5, we would need to repartition all the customers and move them around within the databases. There are many ways to address this issue, but it would be nice to come up with a solution that:<br/><br/><ol><li>Is simple and inexpensive to implement on day one.</li><li>Allows managing of costs as the business grows and the available budget increases.</li><li>Solves the single point of failure issue.</li><li>Allows changing database technology without reengineering the system.</li><li>Improves the ability to test the system.</li></ol><br/>The solution we adopted at one place I worked is based on a physical storage abstraction—we call it “DataBag.” It is designed to separate the physical storage from the data consumers with a very simple NoSql API compatible with memcached. Each physical storage type that is supported is implemented as a plugin of the DataBag system and can be switched depending on needs. For example, in production we can choose to use a different storage solution than in the test or development environments. <br/><br/>Using three storage plugins as an example (MySql, DynamoDB, and a simple Flat File), the system looks like this:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><a class="external-link" href="http://1.bp.blogspot.com/-6bgTMiNTf2I/U0woVewv4cI/AAAAAAAAAvI/xvQOIlINmZ4/s1600/fig6.png" rel="nofollow"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="250" src="https://1.bp.blogspot.com/-6bgTMiNTf2I/U0woVewv4cI/AAAAAAAAAvI/xvQOIlINmZ4/s1600/fig6.png" data-image-src="https://1.bp.blogspot.com/-6bgTMiNTf2I/U0woVewv4cI/AAAAAAAAAvI/xvQOIlINmZ4/s1600/fig6.png"></span></a></div></td></tr><tr><td class="tr-caption confluenceTd">fig6</td></tr></tbody></table></div></div>This solution addresses the various concerns mentioned above:<br/><br/><ol><li>It is inexpensive to implement on day one because the DataBag API is very simple and creating the first plugin is easy.</li><li>It allows tuning the costs because some storage solutions cost more than others. At my former company we started using a MySql implementation and eventually moved to AWS DynamoDB as budget constraints changed. </li><li>The logic to deal with a single point of failure issue is contained in the plugins and is specific to the storage system, but it is abstracted from the rest of the system, so it can be implemented and then refined over time. For example, a first implementation of a single MySql instance plugin can be easily switched with a partitioned MySql plugin solution or with an AWS managed service such as DynamoDB. This abstracts the single point of failure issue and allows refining the handling over time.</li><li>Switching database technology is as easy as implementing one of the plugins and can be completed as an iterative process of experimentation, testing, and refinement. </li><li>Some plugins, such as the flat file version, make it very easy to run tests in a controlled, inexpensive environment. </li></ol><br/>In addition, the DataBag model can be enhanced to automatically and transparently deal with multi-storage solutions. For example, at many companies, data is stored in DynamoDB, we back it up on S3, and we have low-performance mirrors that can be used for easy reporting on MySql. All of this becomes an implementation detail of the DataBag abstraction. We even generalized the migration from a storage type to another with a high-level concept of migration—both lazy and proactive—between data storages. We were also able to generalize the concept of caching by implementing a DataBag based on ElastiCache. All of this is handled in the DataBag subsystem with no dependencies to the rest of the system.<br/><br/>The DataBag solution in this example made the storage itself scalable and fault-tolerant, so our system now looks like this:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://1.bp.blogspot.com/-pgfkZM0pNQk/U0wojOiyHQI/AAAAAAAAAvQ/xoIp5x2LqoE/s1600/fig7.png" data-image-src="https://1.bp.blogspot.com/-pgfkZM0pNQk/U0wojOiyHQI/AAAAAAAAAvQ/xoIp5x2LqoE/s1600/fig7.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig7</td></tr></tbody></table></div></div><div class="separator">Moving our attention backwards, closer to the customer, we encounter another single point of failure: one single physical server running all the logic. If that server fails, our services become unavailable. <br/><br/>This picture represents that grim situation:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://1.bp.blogspot.com/-Wgz062SJ0rI/U0wov71Yr-I/AAAAAAAAAvY/Jz-G3rZUD98/s1600/fig8.png" data-image-src="https://1.bp.blogspot.com/-Wgz062SJ0rI/U0wov71Yr-I/AAAAAAAAAvY/Jz-G3rZUD98/s1600/fig8.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig8</td></tr></tbody></table></div></div><div class="separator">In addition, this single machine running the entire product doesn’t scale because as traffic increases, it will eventually become saturated. An easy solution is to run the product on a cluster of servers with a load balancer in front of it. At my former company we use the AWS Elastic Load Balancer. <br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="250" src="https://3.bp.blogspot.com/-rB602wIPEBY/U0wo471YDEI/AAAAAAAAAvg/ogYrmfVyq9A/s1600/fig9.png" data-image-src="https://3.bp.blogspot.com/-rB602wIPEBY/U0wo471YDEI/AAAAAAAAAvg/ogYrmfVyq9A/s1600/fig9.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig9</td></tr></tbody></table></div></div><div class="separator">The ELB takes care of performing auto-scaling and load balancing of instances in the cluster and also deals with killing unresponsive instances. With this design choice, the system can now be represented as follows:<br/><br/><br/><div class="table-wrap"><br/><div class="table-wrap"><table class="tr-caption-container wrapped confluenceTable" align="center"><colgroup><col/></colgroup><tbody><tr><td class="confluenceTd"><div class="content-wrapper"><span class="confluence-embedded-file-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image confluence-external-resource" height="150" src="https://3.bp.blogspot.com/-unv0N83w2xc/U0wpC3MIjMI/AAAAAAAAAvo/OcO6lhobiXE/s1600/fig10.png" data-image-src="https://3.bp.blogspot.com/-unv0N83w2xc/U0wpC3MIjMI/AAAAAAAAAvo/OcO6lhobiXE/s1600/fig10.png"></span></div></td></tr><tr><td class="tr-caption confluenceTd">fig10</td></tr></tbody></table></div></div><div class="separator">This completes the process of making the initial simple but very fragile system into a fault tolerant and scalable solution, built for failure and for growth. For scalability you would also want to break down the product into multiple services, each of them with an API and running on a cluster with a load balancer in front of it. <br/><br/>We didn’t address the concept of AWS Availability Zones and Regions in this article, but I’ll make sure to discuss those options in future articles.</div><h2 class="separator" id="DesignforFailure-AdditionalReading:">Additional Reading:</h2><div class="separator"><ul><li><a class="external-link" href="https://www.slideshare.net/AmazonWebServices/best-practices-for-architecting-in-the-cloud-jeff-barr/10-Design_for_Failure_with_AWS" rel="nofollow">https://www.slideshare.net/AmazonWebServices/best-practices-for-architecting-in-the-cloud-jeff-barr/10-Design_for_Failure_with_AWS</a></li><li><a class="external-link" href="https://www.lynda.com/Amazon-Web-Services-tutorials/Design-failure/569195/617952-4.html" rel="nofollow">https://www.lynda.com/Amazon-Web-Services-tutorials/Design-failure/569195/617952-4.html</a></li><li><a class="external-link" href="https://www.wired.com/2011/04/lessons-amazon-cloud-failure/" rel="nofollow">https://www.wired.com/2011/04/lessons-amazon-cloud-failure/</a></li><li><a class="external-link" href="https://www.datapipe.com/blog/2016/05/20/understanding-aws-design-principles/" rel="nofollow">https://www.datapipe.com/blog/2016/05/20/understanding-aws-design-principles/</a></li><li><a class="external-link" href="https://media.amazonwebservices.com/AWS_Cloud_Best_Practices.pdf" rel="nofollow">https://media.amazonwebservices.com/AWS_Cloud_Best_Practices.pdf</a></li></ul></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell aside" data-type="aside">
<div class="innerCell">
<h3 id="DesignforFailure-TableofContents">Table of Contents</h3><p><style type='text/css'>/*<![CDATA[*/
div.rbtoc1509255683728 {padding: 0px;}
div.rbtoc1509255683728 ul {list-style: disc;margin-left: 0px;}
div.rbtoc1509255683728 li {margin-left: 0px;padding-left: 0px;}

/*]]>*/</style><div class='toc-macro rbtoc1509255683728'>
<ul class='toc-indentation'>
<li><a href='#DesignforFailure-EverythingFails'>Everything Fails</a></li>
<li><a href='#DesignforFailure-WhyBuildResilientApplications?'>Why Build Resilient Applications?</a></li>
<li><a href='#DesignforFailure-CorePrinciplesforFailure'>Core Principles for Failure</a>
<ul class='toc-indentation'>
<li><a href='#DesignforFailure-GracefulDegredation'>Graceful Degredation</a>
<ul class='toc-indentation'>
<li><a href='#DesignforFailure-Caching'>Caching</a></li>
<li><a href='#DesignforFailure-Retry'>Retry</a></li>
<li><a href='#DesignforFailure-RateLimiting'>Rate Limiting</a></li>
<li><a href='#DesignforFailure-FailFast'>Fail Fast</a></li>
<li><a href='#DesignforFailure-StaticFallback'>Static Fallback</a></li>
<li><a href='#DesignforFailure-FailSilently'>Fail Silently</a></li>
</ul>
</li>
<li><a href='#DesignforFailure-FailingFastandYou'>Failing Fast and You</a>
<ul class='toc-indentation'>
<li><a href='#DesignforFailure-CircuitBreaking'>Circuit Breaking</a></li>
<li><a href='#DesignforFailure-LoadShedding'>Load Shedding</a></li>
</ul>
</li>
</ul>
</li>
<li><a href='#DesignforFailure-Sample(althoughsimplistic)Scenario'>Sample (although simplistic) Scenario</a></li>
<li><a href='#DesignforFailure-AdditionalReading:'>Additional Reading:</a>
<ul class='toc-indentation'>
<li><a href='#DesignforFailure-TableofContents'>Table of Contents</a></li>
</ul>
</li>
</ul>
</div></p></div>
</div>
</div>
</div>
                    </div>

                    
                                                      
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on Oct 29, 2017 01:41</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
